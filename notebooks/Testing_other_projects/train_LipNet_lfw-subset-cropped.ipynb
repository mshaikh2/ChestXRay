{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "from scipy import ndimage\n",
    "from keras.optimizers import Adadelta,Adam\n",
    "from keras.callbacks import TensorBoard, CSVLogger, ModelCheckpoint\n",
    "# from lipnet.lipreading.generators import BasicGenerator\n",
    "# from lipnet.lipreading.callbacks import Statistics, Visualize\n",
    "# from lipnet.lipreading.curriculums import Curriculum\n",
    "# from lipnet.core.decoders import Decoder\n",
    "# from lipnet.lipreading.helpers import labels_to_text\n",
    "# from lipnet.utils.spell import Spell\n",
    "from lipnet.model2 import LipNet\n",
    "import numpy as np\n",
    "import datetime\n",
    "import cv2\n",
    "import skvideo.io\n",
    "import dlib\n",
    "from scipy.misc import imresize\n",
    "# from lipnet.lipreading.aligns import Align\n",
    "import imageio\n",
    "from matplotlib import pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVERYONE', 'EVENTS', 'EVERYBODY', 'CLOUD', 'RIGHTS', 'AUTHORITIES', 'WITHIN', 'SEVERAL', 'ABSOLUTELY', 'WINDS', 'AFTERNOON', 'BETWEEN', 'TALKING', 'AGREEMENT', 'PRICES', 'MEETING', 'AFFAIRS', 'SERIOUS', 'SUNSHINE', 'OPPOSITION']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "tf_config = tf.ConfigProto(allow_soft_placement=False)\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "s = tf.Session(config=tf_config)\n",
    "K.set_session(s)\n",
    "\n",
    "FACE_PREDICTOR_PATH  = os.path.join('common','predictors','shape_predictor_68_face_landmarks.dat')\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(FACE_PREDICTOR_PATH)\n",
    "\n",
    "frames_count = 30\n",
    "words_folder = 'training/unseen_speakers/TARGET/datasets/lipread_mp4'\n",
    "words = os.listdir(words_folder)\n",
    "# words = ['EVERYONE',\n",
    "#  'EVENTS',\n",
    "#  'EVERYBODY',\n",
    "#  'CLOUD',\n",
    "#  'RIGHTS',\n",
    "#  'AUTHORITIES',\n",
    "#  'WITHIN',\n",
    "#  'SEVERAL',\n",
    "#  'ABSOLUTELY',\n",
    "#  'WINDS',\n",
    "#  'AFTERNOON',\n",
    "#  'BETWEEN',\n",
    "#  'TALKING',\n",
    "#  'AGREEMENT',\n",
    "#  'PRICES'] #words[:2]\n",
    "words = [s.upper() for s in words]\n",
    "max_len = 12 #np.max([len(word) for word in words])\n",
    "used_words = {''}\n",
    "print(words)\n",
    "def datagen(data_partition = 'train', batch_size = 16):\n",
    "    while True:\n",
    "        counter = 0\n",
    "        \n",
    "        X_data = []#np.zeros((batch_size,))\n",
    "        Y_data = []\n",
    "        input_length = []\n",
    "        label_length = []\n",
    "        source_str = []\n",
    "        while counter<batch_size:\n",
    "#             print(counter)\n",
    "            word_idx = np.random.randint(len(words))\n",
    "            used_words.add(words[word_idx])\n",
    "            train_folder = 'training/unseen_speakers/TARGET/datasets/lipread_mp4/{0}/{1}/'.format(words[word_idx],data_partition)\n",
    "        #         print(train_folder)\n",
    "            files = os.listdir(train_folder)\n",
    "        #         print(files)\n",
    "            paths = list(set([x.split('.')[0] for x in files]))\n",
    "        \n",
    "            idx = np.random.randint(len(paths))\n",
    "            path = os.path.join(train_folder,paths[idx])\n",
    "            frames = []\n",
    "            for img_name in sorted(os.listdir(path)):\n",
    "                img = cv2.imread(os.path.join(path,img_name))\n",
    "                frames.append(img)\n",
    "            frames = np.array(frames)\n",
    "            if len(frames.shape)==4:\n",
    "                frames = np.rollaxis(frames,2, start=1)\n",
    "    #             print(path)\n",
    "    #             print(frames.shape)\n",
    "    #             print('---------')\n",
    "                if 'numpy.ndarray' in str(type(frames)) \\\n",
    "                        and len(np.shape(frames))==4 \\\n",
    "                        and np.shape(frames)[1] == 100 \\\n",
    "                        and np.shape(frames)[2] == 50:\n",
    "\n",
    "                    X_data.append(frames)\n",
    "\n",
    "\n",
    "                    ####\n",
    "                    Y_data.append([ord(c) - 65 for c in words[word_idx]]+[-1]*(max_len-len(words[word_idx])))\n",
    "                    input_length.append(frames_count)\n",
    "                    label_length.append(len(words[word_idx]))\n",
    "                    source_str.append(words[word_idx])\n",
    "                    counter+=1\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                print('error:', frames.shape, path)\n",
    "                continue\n",
    "\n",
    "        X_data = np.array(X_data).astype(np.float32) / 255 \n",
    "        Y_data = np.array(Y_data)\n",
    "        input_length = np.array(input_length)\n",
    "        label_length = np.array(label_length)\n",
    "        source_str = np.array(source_str)\n",
    "        \n",
    "        inputs = {'the_input': X_data,\n",
    "                  'the_labels': Y_data,\n",
    "                  'input_length': input_length,\n",
    "                  'label_length': label_length,\n",
    "                  'source_str': source_str  # used for visualization only\n",
    "                  }\n",
    "        outputs = {'ctc': np.zeros([batch_size])}\n",
    "        \n",
    "#         print('the_input:', inputs['the_input'].shape)\n",
    "#         print('the_labels:', inputs['the_labels'].shape)\n",
    "#         print('input_length:', inputs['input_length'].shape)\n",
    "#         print('label_length:', inputs['label_length'].shape)\n",
    "#         print('source_str:', inputs['source_str'].shape)\n",
    "#         print('batch_frames_shape',X_data.shape, data_partition)\n",
    "        yield (inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred (?, 30, 27)\n"
     ]
    }
   ],
   "source": [
    "# global lip_gen\n",
    "\n",
    "run_name = datetime.datetime.now().strftime('%Y:%m:%d_%H:%M:%S')\n",
    "run_name, start_epoch, stop_epoch, img_c, img_w, img_h, frames_n, absolute_max_string_len, minibatch_size = \\\n",
    "    run_name, 0, 1000, 3, 100, 50, 30, 12, 2\n",
    "\n",
    "\n",
    "\n",
    "CURRENT_PATH = os.getcwd()+'/training/unseen_speakers'\n",
    "DATASET_DIR  = os.path.join(CURRENT_PATH, 'datasets')\n",
    "OUTPUT_DIR   = os.path.join(CURRENT_PATH, 'results')\n",
    "LOG_DIR      = os.path.join(CURRENT_PATH, 'logs')\n",
    "# if not os.path.exists('training/unseen_speakers/results/'+run_name):\n",
    "#     os.makedirs('training/unseen_speakers/results/'+run_name)\n",
    "if not os.path.exists('training/unseen_speakers/logs/'+run_name):\n",
    "    os.makedirs('training/unseen_speakers/logs/'+run_name)\n",
    "\n",
    "PREDICT_GREEDY      = False\n",
    "PREDICT_BEAM_WIDTH  = 200\n",
    "PREDICT_DICTIONARY  = os.path.join(CURRENT_PATH,'..','..','common','dictionaries','lfw.txt')\n",
    "FACE_PREDICTOR_PATH  = os.path.join(CURRENT_PATH,'..','..','common','predictors','shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# def curriculum_rules(epoch):\n",
    "#     return { 'sentence_length': -1, 'flip_probability': 0.5, 'jitter_probability': 0.05 }\n",
    "\n",
    "\n",
    "# # def train(run_name, start_epoch, stop_epoch, img_c, img_w, img_h, frames_n, absolute_max_string_len, minibatch_size):\n",
    "# curriculum = Curriculum(curriculum_rules)\n",
    "# lip_gen = BasicGenerator(dataset_path=DATASET_DIR, #vtype = \"face\", face_predictor_path = FACE_PREDICTOR_PATH,\n",
    "#                             minibatch_size=minibatch_size,\n",
    "#                             img_c=img_c, img_w=img_w, img_h=img_h, frames_n=frames_n,\n",
    "#                             absolute_max_string_len=absolute_max_string_len,\n",
    "#                             curriculum=curriculum, start_epoch=start_epoch).build()\n",
    "# lip_gen_train = datagen(data_partition='train', batch_size=5)\n",
    "# lip_gen_val = datagen(data_partition='val', batch_size=5)\n",
    "\n",
    "lipnet = LipNet(img_c=img_c, img_w=img_w, img_h=img_h, frames_n=frames_n,\n",
    "                        absolute_max_string_len=absolute_max_string_len, output_size=27)\n",
    "# lipnet.summary()\n",
    "# adadelta = Adadelta(lr=0.0001, decay=1e-08, epsilon=1e-08)\n",
    "\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=1e-08)\n",
    "\n",
    "# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n",
    "lipnet.model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# spell = Spell(path=PREDICT_DICTIONARY)\n",
    "# decoder = Decoder(greedy=PREDICT_GREEDY, beam_width=PREDICT_BEAM_WIDTH,\n",
    "#                   postprocessors=[labels_to_text, spell.sentence])\n",
    "\n",
    "# define callbacks\n",
    "# statistics  = Statistics(lipnet, lip_gen.next_val(), decoder, 256, output_dir=os.path.join(OUTPUT_DIR, run_name))\n",
    "# visualize   = Visualize(os.path.join(OUTPUT_DIR, run_name), lipnet, lip_gen.next_val(), decoder, num_display_sentences=minibatch_size)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(LOG_DIR, 'crop-'+run_name))\n",
    "# csv_logger  = CSVLogger(os.path.join(LOG_DIR, \"{}-{}.csv\".format('training',run_name)), separator=',', append=True)\n",
    "checkpoint  = ModelCheckpoint(save_best_only=True,\n",
    "                              filepath='training/unseen_speakers/results/weights_lfw-crop-v2.hdf5'\n",
    "                              , monitor='val_loss'\n",
    "                              , verbose=1\n",
    "                              , mode='min'\n",
    "                              , period=1)\n",
    "    \n",
    "    # x = next(lip_gen.next_val())\n",
    "    # for i in x:\n",
    "    #     print('$$$$$----------', type(i))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S')\n",
    "# train(run_name, 0, 1000, 3, 100, 50, 75, 32, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.getcwd()+'training/unseen_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g=lip_gen.next_train()\n",
    "# inputs, outputs = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len('place green at t six now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len([ 15.,  11.,   0.,   2.,   4.,  26.,   6.,  17.,   4.,   4.,  13.,\n",
    "#           26.,   0.,  19.,  26.,  19.,  26.,  18.,   8.,  23.,  26.,  13.,\n",
    "#           14.,  22.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.,  -1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100001: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100002: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100003: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100004: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100005: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100006: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100007: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100008: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100009: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100010: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100011: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100012: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100013: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100014: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100015: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100016: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100017: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100018: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100019: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100020: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100021: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100022: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100023: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100024: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100025: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100026: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100027: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100028: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100029: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100030: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100031: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100032: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100033: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100034: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100035: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100036: val_loss did not improve from 0.00208\n",
      "error: (30,) training/unseen_speakers/TARGET/datasets/lipread_mp4/TALKING/train/TALKING_00383\n",
      "\n",
      "Epoch 100037: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100038: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100039: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100040: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100041: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100042: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100043: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100044: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100045: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100046: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100047: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100048: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100049: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100050: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100051: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100052: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100053: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100054: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100055: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100056: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100057: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100058: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100059: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100060: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100061: val_loss did not improve from 0.00208\n",
      "error: (30,) training/unseen_speakers/TARGET/datasets/lipread_mp4/AUTHORITIES/train/AUTHORITIES_00912\n",
      "\n",
      "Epoch 100062: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100063: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100064: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100065: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100066: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100067: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100068: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100069: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100070: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100071: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100072: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100073: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100074: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100075: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100076: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100077: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100078: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100079: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100080: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100081: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100082: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100083: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100084: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100085: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100086: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100087: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100088: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100089: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100090: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100091: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100092: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100093: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100094: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100095: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100096: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100097: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100098: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100099: val_loss did not improve from 0.00208\n",
      "\n",
      "Epoch 100100: val_loss did not improve from 0.00208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f13c269c470>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load weight if necessary\n",
    "\n",
    "start_epoch = 100000\n",
    "if start_epoch > 0:\n",
    "    weight_file = os.path.join(OUTPUT_DIR, 'weights_lfw-crop-v2.hdf5')\n",
    "    lipnet.model.load_weights(weight_file)\n",
    "\n",
    "lipnet.model.fit_generator(generator=datagen(data_partition='train', batch_size=32),\n",
    "                    steps_per_epoch=1\n",
    "                    , epochs=100100,\n",
    "                    #steps_per_epoch=lip_gen.default_training_steps, epochs=stop_epoch,\n",
    "                    validation_data=datagen(data_partition='val', batch_size=32)\n",
    "                    , validation_steps=1,#validation_steps=lip_gen.default_validation_steps,\n",
    "                    callbacks=[checkpoint,tensorboard],                         \n",
    "                    initial_epoch=start_epoch, \n",
    "                    verbose=0)\n",
    "                    #max_q_size=5,\n",
    "                    #workers=2,\n",
    "                    #pickle_safe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# g=datagen(data_partition='test', batch_size=1)\n",
    "# x,y = next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# used_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x['the_input'][0].shape, x['source_str'], x['the_labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # im = batch_frames[1][25]\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# # generator = datagen(min_row_number=0,max_row_number=200000, batch_size= 6)\n",
    "# # X,Y = next(generator)\n",
    "# X = x['the_input'][0]\n",
    "# # print(X.shape, Y.shape)\n",
    "# def plot_data(x,ax):\n",
    "# #     x = x.reshape((size,size))\n",
    "#     ax.imshow(x, cmap='gray')\n",
    "# #     if y is not None:\n",
    "# #         ax.scatter(y[0::2] , y[1::2] , marker='x', s=10)\n",
    "# fig = plt.figure(figsize=(30,30))\n",
    "# fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "# for i in range(30):\n",
    "#     ax = fig.add_subplot(1, 30, i + 1, xticks=[], yticks=[])\n",
    "#     plot_data(X[i], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import io\n",
    "# # import base64\n",
    "# # from IPython.display import HTML\n",
    "\n",
    "# # video = io.open('evaluation/samples/test.mp4', 'r+b').read()\n",
    "# # encoded = base64.b64encode(video)\n",
    "# # HTML(data='''<video alt=\"test\" controls>\n",
    "# #                 <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "# #              </video>'''.format(encoded.decode('ascii')))\n",
    "# im = cv2.imread('training/unseen_speakers/test_pic.jpg')\n",
    "# ims = get_frames_mouth(detector, predictor, [im])\n",
    "# plt.imshow(ims[0][...,::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # np.max([len(word) for word in words])\n",
    "# type(np.array([1,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lipnet.predict(np.array([X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = []\n",
    "# m.append([[1,1],[1,1],[1,1]])\n",
    "# m.append([[1,1],[1,1],[1,1]])\n",
    "# m.append([[1,1],[1,1],[1,1]])\n",
    "# m.append([[1,1],[1,1],[1,1]])\n",
    "# m.append([[1,1],[1,1],[1,1]])\n",
    "# m.append(1)\n",
    "# # m.append([1,1,1])\n",
    "# # m.append([1,1,1])\n",
    "# # m.append([1,1,1])\n",
    "# # m.append([1,1,1])\n",
    "# # m.append([[1],[1],[1]])\n",
    "# np.array(m).shape\n",
    "t = {1,2,3,4,5}\n",
    "t.add(5)\n",
    "t.add(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4, 5, 6}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
