{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model,multi_gpu_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input,GlobalAveragePooling2D,Layer,InputSpec\n",
    "from keras.layers.core import Dense,Flatten,Dropout\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "import keras.layers as kl\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pickle\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# from AttentionModule import SelfAttention, SoftAttention\n",
    "import os\n",
    "visible_gpu_devices = '0'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=visible_gpu_devices\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import AttentionMed as AM\n",
    "from importlib import reload\n",
    "reload(AM)\n",
    "from time import time,localtime,strftime\n",
    "from ipywidgets import IntProgress\n",
    "# from coord import CoordinateChannel2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu=len(visible_gpu_devices.split(','))\n",
    "n_cpu=1\n",
    "tf_config= tf.ConfigProto(device_count = {'GPU': n_gpu , 'CPU': n_cpu})\n",
    "tf_config.gpu_options.allow_growth=True\n",
    "s=tf.Session(config=tf_config)\n",
    "K.set_session(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 40\n",
    "vocab_size = 100\n",
    "embedding_size = 300\n",
    "elu_alpha = 1.0\n",
    "bottleneck_units = 512\n",
    "batchsize = 256\n",
    "def temp_datagen(batchsize=1):\n",
    "    counter=0\n",
    "    while True:\n",
    "        inp = np.random.randint(1,vocab_size,(batchsize,timesteps,))\n",
    "        out = inp.copy()\n",
    "        out[:,::2]=0\n",
    "        out = np_utils.to_categorical(out,num_classes=101)\n",
    "        counter+=1\n",
    "        if batchsize==counter:\n",
    "            yield inp,out\n",
    "            counter=0\n",
    "t_datagen = temp_datagen(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[45, 84, 69, 18, 69, 94, 10, 88, 39,  9, 36, 70, 28, 83, 51, 42,\n",
       "         60, 32, 90,  7, 86, 10,  9, 57, 84, 21, 81, 85, 74,  4,  7, 69,\n",
       "         99, 95, 31, 85, 57, 17, 99, 80],\n",
       "        [83,  7, 65, 29, 93, 67, 67, 17, 93, 96,  6,  1, 68, 17, 57, 61,\n",
       "         13, 89, 72, 18, 89, 68, 19,  8, 57, 72, 34, 60, 53, 29, 99, 42,\n",
       "         50, 81, 70, 45, 54, 45, 34, 73]]),\n",
       " array([[ 0, 84,  0, ..., 17,  0, 80],\n",
       "        [ 0,  7,  0, ..., 45,  0, 73],\n",
       "        [ 0, 88,  0, ..., 16,  0, 78],\n",
       "        ...,\n",
       "        [ 0, 46,  0, ..., 36,  0, 23],\n",
       "        [ 0, 55,  0, ..., 60,  0, 83],\n",
       "        [ 0, 37,  0, ..., 62,  0,  2]]),\n",
       " (256, 40, 101))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x,t_y=next(t_datagen)\n",
    "t_x[:2],t_y.argmax(-1),t_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "w2v_emb (Embedding)             (None, 40, 300)      30300       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 40, 512)      1665024     w2v_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gap (GlobalAveragePooling1D)    (None, 512)          0           lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "a1 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "a2 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "a3 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "a4 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "a5 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "a6 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "a7 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "a8 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 lstm_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 1, 512)       2048        a1[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1, 512)       2048        a2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1, 512)       2048        a3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1, 512)       2048        a4[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 1, 512)       2048        a5[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 1, 512)       2048        a6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 1, 512)       2048        a7[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 1, 512)       2048        a8[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "concat_attn (Concatenate)       (None, 1, 4096)      0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "condense_attention1d_5 (Condens (None, 1, 512)       2097664     concat_attn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_squeeze (Lambda)      (None, 512)          0           condense_attention1d_5[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "residual_combine (ResidualCombi [(None, 512), (1,),  2           gap[0][0]                        \n",
      "                                                                 attention_squeeze[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_4 (RepeatVector)  (None, 40, 512)      0           residual_combine[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 40, 512)      2099200     repeat_vector_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 40, 101)      51813       lstm_9[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 14,365,379\n",
      "Trainable params: 14,357,187\n",
      "Non-trainable params: 8,192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "units = 512\n",
    "\n",
    "inp = kl.Input(shape=(timesteps,))\n",
    "emb = kl.Embedding(vocab_size+1, embedding_size, mask_zero=False, name='w2v_emb')(inp)\n",
    "encoder = kl.LSTM(units,activation='relu',return_sequences=True)(emb)\n",
    "\n",
    "# encoder,sa_maps = AM.SelfAttention(ch=512)(encoder)\n",
    "\n",
    "# context_vector = kl.LSTM(512,activation='relu',return_sequences=False)(encoder)\n",
    "context_vector = kl.GlobalAveragePooling1D(name='gap')(encoder)\n",
    "\n",
    "acv_1,a_map_1 = AM.Attention(ch=512,timesteps=1,name='a1')([context_vector,encoder])\n",
    "acv_1 = kl.BatchNormalization()(acv_1)\n",
    "acv_2,a_map_2 = AM.Attention(ch=512,timesteps=1,name='a2')([context_vector,encoder])\n",
    "acv_2 = kl.BatchNormalization()(acv_2)\n",
    "acv_3,a_map_3 = AM.Attention(ch=512,timesteps=1,name='a3')([context_vector,encoder])\n",
    "acv_3 = kl.BatchNormalization()(acv_3)\n",
    "acv_4,a_map_4 = AM.Attention(ch=512,timesteps=1,name='a4')([context_vector,encoder])\n",
    "acv_4 = kl.BatchNormalization()(acv_4)\n",
    "acv_5,a_map_5 = AM.Attention(ch=512,timesteps=1,name='a5')([context_vector,encoder])\n",
    "acv_5 = kl.BatchNormalization()(acv_5)\n",
    "acv_6,a_map_6 = AM.Attention(ch=512,timesteps=1,name='a6')([context_vector,encoder])\n",
    "acv_6 = kl.BatchNormalization()(acv_6)\n",
    "acv_7,a_map_7 = AM.Attention(ch=512,timesteps=1,name='a7')([context_vector,encoder])\n",
    "acv_7 = kl.BatchNormalization()(acv_7)\n",
    "acv_8,a_map_8 = AM.Attention(ch=512,timesteps=1,name='a8')([context_vector,encoder])\n",
    "acv_8 = kl.BatchNormalization()(acv_8)\n",
    "\n",
    "concat_attention = kl.Concatenate(name='concat_attn')([acv_1,acv_2,acv_3,acv_4,acv_5,acv_6,acv_7,acv_8])\n",
    "concat_attention = AM.CondenseAttention1D(ch_in=int(concat_attention.shape[-1]),ch_out=512)(concat_attention)\n",
    "concat_attention = kl.Lambda(lambda x:K.squeeze(x,axis=1),name=\"attention_squeeze\")(concat_attention)\n",
    "concat_attention, g1, g2 = AM.ResidualCombine(method='add',name='residual_combine')([context_vector, concat_attention])\n",
    "\n",
    "# acv_combine = kl.ELU(alpha=elu_alpha)(concat_attention)\n",
    "# acv_combine = kl.Lambda(lambda x:K.expand_dims(x,axis=1),name=\"Expand\")(acv_combine)\n",
    "# acv_combine = kl.Conv1D(filters=bottleneck_units,kernel_size=1,strides=1,padding='same')(acv_combine)\n",
    "# acv_combine = kl.BatchNormalization()(acv_combine)\n",
    "# acv_combine = kl.Lambda(lambda x:K.squeeze(x,axis=1),name=\"Squeeze2\")(acv_combine)\n",
    "\n",
    "# # # # print(attended_context_vector)\n",
    "repeat_context_vector = kl.RepeatVector(n=timesteps)(concat_attention)\n",
    "decoder = kl.LSTM(512,activation='relu',return_sequences=True)(repeat_context_vector)\n",
    "# # AM.Attention(ch=1)([])\n",
    "decoder_out = kl.TimeDistributed(Dense(vocab_size+1,activation='softmax'))(decoder)\n",
    "model = Model(inp,decoder_out)\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=Adam(lr=0.001))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "50/50 [==============================] - 27s 535ms/step - loss: 3.6136 - acc: 0.4901\n",
      "Epoch 2/1000\n",
      "50/50 [==============================] - 10s 192ms/step - loss: 2.6884 - acc: 0.5021\n",
      "Epoch 3/1000\n",
      "50/50 [==============================] - 10s 202ms/step - loss: 2.3001 - acc: 0.5066\n",
      "Epoch 4/1000\n",
      "50/50 [==============================] - 10s 204ms/step - loss: 2.2809 - acc: 0.5101\n",
      "Epoch 5/1000\n",
      "50/50 [==============================] - 11s 225ms/step - loss: 2.2661 - acc: 0.5118\n",
      "Epoch 6/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.2550 - acc: 0.5135\n",
      "Epoch 7/1000\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 2.2456 - acc: 0.5145\n",
      "Epoch 8/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.2366 - acc: 0.5150\n",
      "Epoch 9/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 2.2300 - acc: 0.5158\n",
      "Epoch 10/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.2270 - acc: 0.5164\n",
      "Epoch 11/1000\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 2.2199 - acc: 0.5172\n",
      "Epoch 12/1000\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 2.2137 - acc: 0.5178\n",
      "Epoch 13/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.2067 - acc: 0.5182\n",
      "Epoch 14/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.1974 - acc: 0.5194\n",
      "Epoch 15/1000\n",
      "50/50 [==============================] - 21s 411ms/step - loss: 2.1872 - acc: 0.5202\n",
      "Epoch 16/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.1806 - acc: 0.5207\n",
      "Epoch 17/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 2.1658 - acc: 0.5219\n",
      "Epoch 18/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.1522 - acc: 0.5225\n",
      "Epoch 19/1000\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 2.1382 - acc: 0.5236\n",
      "Epoch 20/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.1255 - acc: 0.5246\n",
      "Epoch 21/1000\n",
      "50/50 [==============================] - 20s 408ms/step - loss: 2.1169 - acc: 0.5248\n",
      "Epoch 22/1000\n",
      "50/50 [==============================] - 20s 407ms/step - loss: 2.1107 - acc: 0.5253\n",
      "Epoch 23/1000\n",
      "50/50 [==============================] - 21s 411ms/step - loss: 2.1076 - acc: 0.5249\n",
      "Epoch 24/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 2.1020 - acc: 0.5251\n",
      "Epoch 25/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 2.0959 - acc: 0.5257\n",
      "Epoch 26/1000\n",
      "50/50 [==============================] - 20s 406ms/step - loss: 2.0897 - acc: 0.5257\n",
      "Epoch 27/1000\n",
      "50/50 [==============================] - 20s 405ms/step - loss: 2.0833 - acc: 0.5262\n",
      "Epoch 28/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 2.0794 - acc: 0.5260\n",
      "Epoch 29/1000\n",
      "50/50 [==============================] - 20s 405ms/step - loss: 2.0754 - acc: 0.5262\n",
      "Epoch 30/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 2.0661 - acc: 0.5269\n",
      "Epoch 31/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.0596 - acc: 0.5278\n",
      "Epoch 32/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.0557 - acc: 0.5288\n",
      "Epoch 33/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 2.0590 - acc: 0.5278\n",
      "Epoch 34/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 2.0492 - acc: 0.5282\n",
      "Epoch 35/1000\n",
      "50/50 [==============================] - 21s 411ms/step - loss: 2.0411 - acc: 0.5289\n",
      "Epoch 36/1000\n",
      "50/50 [==============================] - 20s 408ms/step - loss: 2.0275 - acc: 0.5310\n",
      "Epoch 37/1000\n",
      "50/50 [==============================] - 20s 405ms/step - loss: 2.0333 - acc: 0.5364\n",
      "Epoch 38/1000\n",
      "50/50 [==============================] - 21s 411ms/step - loss: 2.0016 - acc: 0.5416\n",
      "Epoch 39/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.9589 - acc: 0.5487\n",
      "Epoch 40/1000\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 1.9438 - acc: 0.5504\n",
      "Epoch 41/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 1.9314 - acc: 0.5514\n",
      "Epoch 42/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.9294 - acc: 0.5516\n",
      "Epoch 43/1000\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 1.9250 - acc: 0.5522\n",
      "Epoch 44/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.7783 - acc: 0.5614\n",
      "Epoch 45/1000\n",
      "50/50 [==============================] - 21s 411ms/step - loss: 1.6908 - acc: 0.5660\n",
      "Epoch 46/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.6782 - acc: 0.5659\n",
      "Epoch 47/1000\n",
      "50/50 [==============================] - 21s 411ms/step - loss: 1.6708 - acc: 0.5661\n",
      "Epoch 48/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.6648 - acc: 0.5660\n",
      "Epoch 49/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.6600 - acc: 0.5657\n",
      "Epoch 50/1000\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 1.6552 - acc: 0.5660\n",
      "Epoch 51/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.6523 - acc: 0.5655\n",
      "Epoch 52/1000\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 1.6491 - acc: 0.5653\n",
      "Epoch 53/1000\n",
      "50/50 [==============================] - 21s 413ms/step - loss: 1.6464 - acc: 0.5653\n",
      "Epoch 54/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 1.6417 - acc: 0.5652\n",
      "Epoch 55/1000\n",
      "50/50 [==============================] - 21s 410ms/step - loss: 1.6388 - acc: 0.5650\n",
      "Epoch 56/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 1.6356 - acc: 0.5649\n",
      "Epoch 57/1000\n",
      "50/50 [==============================] - 20s 410ms/step - loss: 1.6361 - acc: 0.5647\n",
      "Epoch 58/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.6340 - acc: 0.5646\n",
      "Epoch 59/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.6281 - acc: 0.5647\n",
      "Epoch 60/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.6283 - acc: 0.5646\n",
      "Epoch 61/1000\n",
      "50/50 [==============================] - 21s 411ms/step - loss: 1.6271 - acc: 0.5643\n",
      "Epoch 62/1000\n",
      "50/50 [==============================] - 20s 409ms/step - loss: 1.6257 - acc: 0.5642\n",
      "Epoch 63/1000\n",
      "49/50 [============================>.] - ETA: 0s - loss: 1.6195 - acc: 0.5643"
     ]
    }
   ],
   "source": [
    "# /media/MyDataStor1/mshaikh2/project_xray/notebooks\n",
    "hist = model.fit_generator(t_datagen,epochs=1000,verbose=1,steps_per_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hist.history)\n",
    "df.to_csv(\"graphs/gap_8attn_bn_res_add_metrics.csv\")\n",
    "plot1,plot2 = df.plot(subplots=True, figsize=(5,10))\n",
    "fig1 = plot1.get_figure()\n",
    "fig2 = plot2.get_figure()\n",
    "fig1.savefig(\"graphs/gap_8attn_bn_res_add_acc.png\")\n",
    "fig2.savefig(\"graphs/gap_8attn_bn_res_add_loss.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
