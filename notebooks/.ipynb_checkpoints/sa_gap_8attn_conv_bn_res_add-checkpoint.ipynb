{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model,multi_gpu_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input,GlobalAveragePooling2D,Layer,InputSpec\n",
    "from keras.layers.core import Dense,Flatten,Dropout\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "import keras.layers as kl\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pickle\n",
    "from keras.models import model_from_json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from AttentionModule import SelfAttention, SoftAttention\n",
    "import os\n",
    "visible_gpu_devices = '0,1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=visible_gpu_devices\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import AttentionMed as AM\n",
    "from importlib import reload\n",
    "reload(AM)\n",
    "from time import time,localtime,strftime\n",
    "from ipywidgets import IntProgress\n",
    "# from coord import CoordinateChannel2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu=len(visible_gpu_devices.split(','))\n",
    "n_cpu=1\n",
    "tf_config= tf.ConfigProto(device_count = {'GPU': n_gpu , 'CPU': n_cpu})\n",
    "tf_config.gpu_options.allow_growth=True\n",
    "s=tf.Session(config=tf_config)\n",
    "K.set_session(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiheadSelfAttention(prev_layer,layer_number=0,heads=8):\n",
    "    assert prev_layer != None\n",
    "    sa_arr = []\n",
    "    for head in range(heads):\n",
    "        sa,samap = AM.SelfAttention(ch=int(prev_layer.shape[-1]),name='sa{0}{1}'.format(layer_number,head))(prev_layer)\n",
    "        sa = kl.BatchNormalization()(sa)\n",
    "        sa_arr.append(sa)\n",
    "    return sa_arr\n",
    "def multiheadAttention(prev_layer,context_vector,layer_number=0,heads=8):\n",
    "    assert prev_layer != None\n",
    "    assert context_vector != None\n",
    "    a_arr = []\n",
    "    for head in range(heads):\n",
    "        a,a_map = AM.Attention(ch=int(prev_layer.shape[-1]),timesteps=1,name='a{0}{1}'.format(layer_number,head))([context_vector,prev_layer])\n",
    "        a = kl.BatchNormalization()(a)\n",
    "        a_arr.append(a)\n",
    "    return a_arr\n",
    "def feedforwardsa(sa_layer,layer_before_sa,out_channels,layer_number=0,method='add'):\n",
    "    x = kl.Concatenate(name='concat_selfattn_{0}'.format(layer_number))(sa_layer)\n",
    "    x = AM.CondenseAttention1D(ch_in=int(x.shape[-1]),ch_out=out_channels)(x)\n",
    "    x, g1, g2 = AM.ResidualCombine(method=method\n",
    "                                   ,name='residual_combine_selfattn_{0}'.format(layer_number))([layer_before_sa, x])\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    return x\n",
    "def feedforwarda(a_layer,context_vector,out_channels,layer_number=0,method='add'):\n",
    "    x = kl.Concatenate(name='concat_attn_{0}'.format(layer_number))(a_layer)\n",
    "    x = AM.CondenseAttention1D(ch_in=int(x.shape[-1]),ch_out=out_channels)(x)\n",
    "    x = kl.Lambda(lambda x:K.squeeze(x,axis=1),name=\"squeeze_attn_{0}\".format(layer_number))(x)\n",
    "    x, g1, g2 = AM.ResidualCombine(method=method\n",
    "                                   ,name='residual_combine_attn_{0}'.format(layer_number))([context_vector, x])\n",
    "    x = kl.BatchNormalization()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 40\n",
    "vocab_size = 100\n",
    "embedding_size = 300\n",
    "elu_alpha = 1.0\n",
    "bottleneck_units = 512\n",
    "batchsize = 256\n",
    "def temp_datagen(batchsize=1):\n",
    "    counter=0\n",
    "    while True:\n",
    "        inp = np.random.randint(1,vocab_size,(batchsize,timesteps,))\n",
    "        out = inp.copy()\n",
    "        out[:,::2]=0\n",
    "        out = np_utils.to_categorical(out,num_classes=101)\n",
    "        counter+=1\n",
    "        if batchsize==counter:\n",
    "            yield inp,out\n",
    "            counter=0\n",
    "t_datagen = temp_datagen(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[82, 40, 53, 36, 11, 82, 58, 17, 52, 90, 58, 21, 77, 67, 17, 14,\n",
       "         74, 86, 81,  3, 18, 62, 45, 52, 40, 47, 13, 21, 23, 11, 48, 90,\n",
       "         80, 51, 86, 30, 77, 80, 89, 64],\n",
       "        [88, 70, 11, 94, 44,  2, 56, 21, 24, 62, 60, 35, 37, 59, 62,  5,\n",
       "         24, 86, 67, 13, 47, 63, 56, 70,  7,  8, 49, 78, 64, 36,  9, 64,\n",
       "         94, 58, 40, 17, 13, 24, 53, 23]]),\n",
       " array([[ 0, 40,  0, ..., 80,  0, 64],\n",
       "        [ 0, 70,  0, ..., 24,  0, 23],\n",
       "        [ 0, 11,  0, ...,  6,  0, 67],\n",
       "        ...,\n",
       "        [ 0, 40,  0, ..., 45,  0, 84],\n",
       "        [ 0, 99,  0, ..., 68,  0, 54],\n",
       "        [ 0, 64,  0, ..., 16,  0, 49]]),\n",
       " (256, 40, 101))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x,t_y=next(t_datagen)\n",
    "t_x[:2],t_y.argmax(-1),t_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "w2v_emb (Embedding)             (None, 40, 300)      30300       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 40, 300)      270300      w2v_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 40, 300)      270300      w2v_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 40, 600)      0           conv1d_1[0][0]                   \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 40, 300)      180300      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 40, 300)      1200        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sa10 (SelfAttention)            [(None, 40, 300), (4 202874      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa11 (SelfAttention)            [(None, 40, 300), (4 202874      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa12 (SelfAttention)            [(None, 40, 300), (4 202874      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa13 (SelfAttention)            [(None, 40, 300), (4 202874      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa14 (SelfAttention)            [(None, 40, 300), (4 202874      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa15 (SelfAttention)            [(None, 40, 300), (4 202874      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa16 (SelfAttention)            [(None, 40, 300), (4 202874      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "sa17 (SelfAttention)            [(None, 40, 300), (4 202874      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 40, 300)      1200        sa10[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 40, 300)      1200        sa11[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 40, 300)      1200        sa12[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 40, 300)      1200        sa13[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 40, 300)      1200        sa14[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 40, 300)      1200        sa15[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 40, 300)      1200        sa16[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 40, 300)      1200        sa17[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "concat_selfattn_1 (Concatenate) (None, 40, 2400)     0           batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "condense_attention1d_1 (Condens (None, 40, 300)      720300      concat_selfattn_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "residual_combine_selfattn_1 (Re [(None, 40, 300), (1 2           batch_normalization_1[0][0]      \n",
      "                                                                 condense_attention1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 40, 300)      1200        residual_combine_selfattn_1[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "gap (GlobalAveragePooling1D)    (None, 300)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "a10 (Attention)                 [(None, 1, 300), (1, 361200      gap[0][0]                        \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "a11 (Attention)                 [(None, 1, 300), (1, 361200      gap[0][0]                        \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "a12 (Attention)                 [(None, 1, 300), (1, 361200      gap[0][0]                        \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "a13 (Attention)                 [(None, 1, 300), (1, 361200      gap[0][0]                        \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "a14 (Attention)                 [(None, 1, 300), (1, 361200      gap[0][0]                        \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "a15 (Attention)                 [(None, 1, 300), (1, 361200      gap[0][0]                        \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "a16 (Attention)                 [(None, 1, 300), (1, 361200      gap[0][0]                        \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "a17 (Attention)                 [(None, 1, 300), (1, 361200      gap[0][0]                        \n",
      "                                                                 batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 1, 300)       1200        a10[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 1, 300)       1200        a11[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 1, 300)       1200        a12[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 1, 300)       1200        a13[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 1, 300)       1200        a14[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 1, 300)       1200        a15[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 1, 300)       1200        a16[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 1, 300)       1200        a17[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concat_attn_1 (Concatenate)     (None, 1, 2400)      0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "                                                                 batch_normalization_14[0][0]     \n",
      "                                                                 batch_normalization_15[0][0]     \n",
      "                                                                 batch_normalization_16[0][0]     \n",
      "                                                                 batch_normalization_17[0][0]     \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "condense_attention1d_2 (Condens (None, 1, 300)       720300      concat_attn_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "squeeze_attn_1 (Lambda)         (None, 300)          0           condense_attention1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "residual_combine_attn_1 (Residu [(None, 300), (1,),  2           gap[0][0]                        \n",
      "                                                                 squeeze_attn_1[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 300)          1200        residual_combine_attn_1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 40, 300)      0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 40, 512)      1665024     repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 40, 101)      51813       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,444,033\n",
      "Trainable params: 8,432,633\n",
      "Non-trainable params: 11,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "units = 512\n",
    "number_sa_layers = 4\n",
    "\n",
    "inp = kl.Input(shape=(timesteps,))\n",
    "emb = kl.Embedding(vocab_size+1, embedding_size, mask_zero=False, name='w2v_emb')(inp)\n",
    "\n",
    "c1 = kl.Conv1D(embedding_size,activation='relu',strides=1,kernel_size=3,padding='same')(emb)\n",
    "c2 = kl.Conv1D(embedding_size,activation='relu',strides=1,kernel_size=3,dilation_rate=2,padding='same')(emb)\n",
    "# c3 = kl.Conv1D(embedding_size,activation='relu',strides=1,kernel_size=5,padding='same')(emb)\n",
    "# c4 = kl.Conv1D(embedding_size,activation='relu',strides=1,kernel_size=5,dilation_rate=2,padding='same')(emb)\n",
    "# c5 = kl.Conv1D(embedding_size,activation='relu',strides=1,kernel_size=7,padding='same')(emb)\n",
    "# c6 = kl.Conv1D(embedding_size,activation='relu',strides=1,kernel_size=7,dilation_rate=2,padding='same')(emb)\n",
    "emb = kl.Concatenate(axis=-1)([c1,c2])\n",
    "emb = kl.Conv1D(embedding_size,activation='relu',strides=1,kernel_size=1,padding='same')(emb)\n",
    "emb = kl.BatchNormalization()(emb)\n",
    "\n",
    "\n",
    "sa = multiheadSelfAttention(prev_layer=emb,layer_number=1,heads=8)\n",
    "sa_ff = feedforwardsa(sa_layer=sa,layer_before_sa=emb,layer_number=1,method='add',out_channels=int(emb.shape[-1]))\n",
    "\n",
    "# sa = multiheadSelfAttention(prev_layer=sa_ff,layer_number=2,heads=8)\n",
    "# sa_ff = feedforwardsa(sa_layer=sa,layer_before_sa=sa_ff,layer_number=2,method='add',out_channels=int(sa_ff.shape[-1]))\n",
    "\n",
    "# sa = multiheadSelfAttention(prev_layer=sa_ff,layer_number=3,heads=8)\n",
    "# sa_ff = feedforwardsa(sa_layer=sa,layer_before_sa=sa_ff,layer_number=3,method='add',out_channels=int(sa_ff.shape[-1]))\n",
    "\n",
    "\n",
    "# context_vector = kl.LSTM(512,activation='relu',return_sequences=False)(encoder)\n",
    "context_vector = kl.GlobalAveragePooling1D(name='gap')(sa_ff)\n",
    "\n",
    "a = multiheadAttention(prev_layer=sa_ff,context_vector=context_vector,layer_number=1,heads=8)\n",
    "a_ff = feedforwarda(a_layer=a,context_vector=context_vector,out_channels=int(emb.shape[-1]),layer_number=1,method='add')\n",
    "\n",
    "repeat_context_vector = kl.RepeatVector(n=timesteps)(a_ff)\n",
    "\n",
    "decoder = kl.LSTM(512,activation='relu',return_sequences=True)(repeat_context_vector)\n",
    "decoder_out = kl.TimeDistributed(Dense(vocab_size+1,activation='softmax'))(decoder)\n",
    "model = Model(inp,decoder_out)\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=Adam(lr=0.0001))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_model=multi_gpu_model(model, gpus=n_gpu)\n",
    "parallel_model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=Adam(lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "50/50 [==============================] - 26s 514ms/step - loss: 3.6399 - acc: 0.4124\n",
      "Epoch 2/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 3.2262 - acc: 0.4940\n",
      "Epoch 3/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 3.1483 - acc: 0.4989\n",
      "Epoch 4/500\n",
      "50/50 [==============================] - 8s 163ms/step - loss: 3.1026 - acc: 0.4999\n",
      "Epoch 5/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 3.0672 - acc: 0.5000\n",
      "Epoch 6/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 3.0396 - acc: 0.5000\n",
      "Epoch 7/500\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 3.0157 - acc: 0.5000\n",
      "Epoch 8/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 2.9941 - acc: 0.5000\n",
      "Epoch 9/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 2.9743 - acc: 0.5000\n",
      "Epoch 10/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 2.9560 - acc: 0.5000\n",
      "Epoch 11/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 2.9380 - acc: 0.5000\n",
      "Epoch 12/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 2.9203 - acc: 0.5000\n",
      "Epoch 13/500\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 2.9017 - acc: 0.5000\n",
      "Epoch 14/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 2.8845 - acc: 0.5000\n",
      "Epoch 15/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 2.8693 - acc: 0.5000\n",
      "Epoch 16/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 2.8547 - acc: 0.5000\n",
      "Epoch 17/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 2.8415 - acc: 0.4999\n",
      "Epoch 18/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 2.8293 - acc: 0.4999\n",
      "Epoch 19/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 2.8170 - acc: 0.5000\n",
      "Epoch 20/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 2.8053 - acc: 0.4999\n",
      "Epoch 21/500\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 2.7950 - acc: 0.5000\n",
      "Epoch 22/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 2.7841 - acc: 0.4999\n",
      "Epoch 23/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 2.7744 - acc: 0.4998\n",
      "Epoch 24/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 2.7657 - acc: 0.4999\n",
      "Epoch 25/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 2.7555 - acc: 0.4999\n",
      "Epoch 26/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 2.7446 - acc: 0.4997\n",
      "Epoch 27/500\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 2.7361 - acc: 0.4999\n",
      "Epoch 28/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 2.7230 - acc: 0.4999\n",
      "Epoch 29/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 2.5925 - acc: 0.5045\n",
      "Epoch 30/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 2.1488 - acc: 0.5246\n",
      "Epoch 31/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 2.0720 - acc: 0.5281\n",
      "Epoch 32/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 2.0420 - acc: 0.5289\n",
      "Epoch 33/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 2.0262 - acc: 0.5299\n",
      "Epoch 34/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 2.0168 - acc: 0.5304\n",
      "Epoch 35/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 2.0082 - acc: 0.5308\n",
      "Epoch 36/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 2.0021 - acc: 0.5307\n",
      "Epoch 37/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9972 - acc: 0.5308\n",
      "Epoch 38/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9933 - acc: 0.5310\n",
      "Epoch 39/500\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 1.9885 - acc: 0.5312\n",
      "Epoch 40/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.9848 - acc: 0.5311\n",
      "Epoch 41/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.9802 - acc: 0.5313\n",
      "Epoch 42/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9784 - acc: 0.5313\n",
      "Epoch 43/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.9753 - acc: 0.5316\n",
      "Epoch 44/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9722 - acc: 0.5318\n",
      "Epoch 45/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9683 - acc: 0.5316\n",
      "Epoch 46/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9665 - acc: 0.5320\n",
      "Epoch 47/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.9646 - acc: 0.5320\n",
      "Epoch 48/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9601 - acc: 0.5322\n",
      "Epoch 49/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.9584 - acc: 0.5319\n",
      "Epoch 50/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9550 - acc: 0.5322\n",
      "Epoch 51/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9534 - acc: 0.5323\n",
      "Epoch 52/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9512 - acc: 0.5321\n",
      "Epoch 53/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.9497 - acc: 0.5322\n",
      "Epoch 54/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.9475 - acc: 0.5320\n",
      "Epoch 55/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.9443 - acc: 0.5323\n",
      "Epoch 56/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9430 - acc: 0.5325\n",
      "Epoch 57/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9417 - acc: 0.5327\n",
      "Epoch 58/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.9396 - acc: 0.5324\n",
      "Epoch 59/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.9382 - acc: 0.5323\n",
      "Epoch 60/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9346 - acc: 0.5326\n",
      "Epoch 61/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.9335 - acc: 0.5321\n",
      "Epoch 62/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.9316 - acc: 0.5324\n",
      "Epoch 63/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.9289 - acc: 0.5327\n",
      "Epoch 64/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.9265 - acc: 0.5326\n",
      "Epoch 65/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9263 - acc: 0.5323\n",
      "Epoch 66/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9237 - acc: 0.5329\n",
      "Epoch 67/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.9220 - acc: 0.5325\n",
      "Epoch 68/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9192 - acc: 0.5324\n",
      "Epoch 69/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.9187 - acc: 0.5327\n",
      "Epoch 70/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9166 - acc: 0.5329\n",
      "Epoch 71/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.9151 - acc: 0.5329\n",
      "Epoch 72/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.9126 - acc: 0.5329\n",
      "Epoch 73/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9105 - acc: 0.5324\n",
      "Epoch 74/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9085 - acc: 0.5327\n",
      "Epoch 75/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.9072 - acc: 0.5327\n",
      "Epoch 76/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.9050 - acc: 0.5326\n",
      "Epoch 77/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.9020 - acc: 0.5327\n",
      "Epoch 78/500\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 1.9012 - acc: 0.5324\n",
      "Epoch 79/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.8990 - acc: 0.5331\n",
      "Epoch 80/500\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 1.8959 - acc: 0.5329\n",
      "Epoch 81/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.8938 - acc: 0.5328\n",
      "Epoch 82/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.8919 - acc: 0.5329\n",
      "Epoch 83/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.8897 - acc: 0.5329\n",
      "Epoch 84/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.8882 - acc: 0.5327\n",
      "Epoch 85/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.8844 - acc: 0.5332\n",
      "Epoch 86/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.8825 - acc: 0.5330\n",
      "Epoch 87/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.8809 - acc: 0.5328\n",
      "Epoch 88/500\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 1.8772 - acc: 0.5329\n",
      "Epoch 89/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.8752 - acc: 0.5333\n",
      "Epoch 90/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.8717 - acc: 0.5332\n",
      "Epoch 91/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.8691 - acc: 0.5328\n",
      "Epoch 92/500\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 1.8679 - acc: 0.5328\n",
      "Epoch 93/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.8638 - acc: 0.5329\n",
      "Epoch 94/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.8604 - acc: 0.5332\n",
      "Epoch 95/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.8593 - acc: 0.5328\n",
      "Epoch 96/500\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 1.8562 - acc: 0.5329\n",
      "Epoch 97/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.8528 - acc: 0.5331\n",
      "Epoch 98/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.8502 - acc: 0.5330\n",
      "Epoch 99/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.8475 - acc: 0.5328\n",
      "Epoch 100/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.8449 - acc: 0.5325\n",
      "Epoch 101/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.8424 - acc: 0.5327\n",
      "Epoch 102/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.8379 - acc: 0.5329\n",
      "Epoch 103/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.8364 - acc: 0.5326\n",
      "Epoch 104/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 1.8329 - acc: 0.5324\n",
      "Epoch 105/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.8294 - acc: 0.5325\n",
      "Epoch 106/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.8256 - acc: 0.5328\n",
      "Epoch 107/500\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 1.8234 - acc: 0.5323\n",
      "Epoch 108/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.8215 - acc: 0.5322\n",
      "Epoch 109/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.8168 - acc: 0.5328\n",
      "Epoch 110/500\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 1.8144 - acc: 0.5326\n",
      "Epoch 111/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.8116 - acc: 0.5325\n",
      "Epoch 112/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.8086 - acc: 0.5324\n",
      "Epoch 113/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.8059 - acc: 0.5321\n",
      "Epoch 114/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.8028 - acc: 0.5326\n",
      "Epoch 115/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.8007 - acc: 0.5321\n",
      "Epoch 116/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.7989 - acc: 0.5322\n",
      "Epoch 117/500\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 1.7951 - acc: 0.5323\n",
      "Epoch 118/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.7921 - acc: 0.5324\n",
      "Epoch 119/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.7894 - acc: 0.5323\n",
      "Epoch 120/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.7862 - acc: 0.5326\n",
      "Epoch 121/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.7838 - acc: 0.5325\n",
      "Epoch 122/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.7802 - acc: 0.5333\n",
      "Epoch 123/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.7796 - acc: 0.5327\n",
      "Epoch 124/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.7762 - acc: 0.5329\n",
      "Epoch 125/500\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 1.7738 - acc: 0.5329\n",
      "Epoch 126/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.7704 - acc: 0.5335\n",
      "Epoch 127/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.7687 - acc: 0.5336\n",
      "Epoch 128/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.7665 - acc: 0.5335\n",
      "Epoch 129/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.7636 - acc: 0.5338\n",
      "Epoch 130/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.7604 - acc: 0.5346\n",
      "Epoch 131/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.7579 - acc: 0.5343\n",
      "Epoch 132/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.7564 - acc: 0.5354\n",
      "Epoch 133/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.7534 - acc: 0.5359\n",
      "Epoch 134/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.7502 - acc: 0.5366\n",
      "Epoch 135/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.7452 - acc: 0.5380\n",
      "Epoch 136/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.7419 - acc: 0.5397\n",
      "Epoch 137/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.7365 - acc: 0.5420\n",
      "Epoch 138/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.7305 - acc: 0.5443\n",
      "Epoch 139/500\n",
      "50/50 [==============================] - 8s 163ms/step - loss: 1.7245 - acc: 0.5471\n",
      "Epoch 140/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.7166 - acc: 0.5499\n",
      "Epoch 141/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.7092 - acc: 0.5520\n",
      "Epoch 142/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.7012 - acc: 0.5543\n",
      "Epoch 143/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.6941 - acc: 0.5564\n",
      "Epoch 144/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.6848 - acc: 0.5589\n",
      "Epoch 145/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.6760 - acc: 0.5617\n",
      "Epoch 146/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.6655 - acc: 0.5646\n",
      "Epoch 147/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.6562 - acc: 0.5672\n",
      "Epoch 148/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.6495 - acc: 0.5691\n",
      "Epoch 149/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.6409 - acc: 0.5710\n",
      "Epoch 150/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.6317 - acc: 0.5733\n",
      "Epoch 151/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.6258 - acc: 0.5737\n",
      "Epoch 152/500\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 1.6187 - acc: 0.5749\n",
      "Epoch 153/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.6133 - acc: 0.5753\n",
      "Epoch 154/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.6089 - acc: 0.5765\n",
      "Epoch 155/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 1.6033 - acc: 0.5769\n",
      "Epoch 156/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5989 - acc: 0.5772\n",
      "Epoch 157/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5945 - acc: 0.5779\n",
      "Epoch 158/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5910 - acc: 0.5780\n",
      "Epoch 159/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5890 - acc: 0.5785\n",
      "Epoch 160/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5856 - acc: 0.5789\n",
      "Epoch 161/500\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 1.5829 - acc: 0.5788\n",
      "Epoch 162/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5806 - acc: 0.5788\n",
      "Epoch 163/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5770 - acc: 0.5794\n",
      "Epoch 164/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5760 - acc: 0.5796\n",
      "Epoch 165/500\n",
      "50/50 [==============================] - 9s 170ms/step - loss: 1.5736 - acc: 0.5799\n",
      "Epoch 166/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5727 - acc: 0.5797\n",
      "Epoch 167/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5700 - acc: 0.5804\n",
      "Epoch 168/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5681 - acc: 0.5801\n",
      "Epoch 169/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5689 - acc: 0.5800\n",
      "Epoch 170/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5664 - acc: 0.5802\n",
      "Epoch 171/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5656 - acc: 0.5803\n",
      "Epoch 172/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5635 - acc: 0.5806\n",
      "Epoch 173/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5639 - acc: 0.5804\n",
      "Epoch 174/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5628 - acc: 0.5805\n",
      "Epoch 175/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5601 - acc: 0.5810\n",
      "Epoch 176/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5607 - acc: 0.5809\n",
      "Epoch 177/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5600 - acc: 0.5807\n",
      "Epoch 178/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5577 - acc: 0.5809\n",
      "Epoch 179/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5568 - acc: 0.5809\n",
      "Epoch 180/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5560 - acc: 0.5810\n",
      "Epoch 181/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5553 - acc: 0.5811\n",
      "Epoch 182/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5548 - acc: 0.5810\n",
      "Epoch 183/500\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 1.5547 - acc: 0.5810\n",
      "Epoch 184/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5546 - acc: 0.5809\n",
      "Epoch 185/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5535 - acc: 0.5813\n",
      "Epoch 186/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5525 - acc: 0.5809\n",
      "Epoch 187/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5522 - acc: 0.5813\n",
      "Epoch 188/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5504 - acc: 0.5812\n",
      "Epoch 189/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5514 - acc: 0.5812\n",
      "Epoch 190/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5502 - acc: 0.5813\n",
      "Epoch 191/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5491 - acc: 0.5816\n",
      "Epoch 192/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 1.5499 - acc: 0.5810\n",
      "Epoch 193/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5481 - acc: 0.5815\n",
      "Epoch 194/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5465 - acc: 0.5818\n",
      "Epoch 195/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5465 - acc: 0.5816\n",
      "Epoch 196/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5459 - acc: 0.5818\n",
      "Epoch 197/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5456 - acc: 0.5818\n",
      "Epoch 198/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5452 - acc: 0.5814\n",
      "Epoch 199/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5443 - acc: 0.5814\n",
      "Epoch 200/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5442 - acc: 0.5816\n",
      "Epoch 201/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5430 - acc: 0.5817\n",
      "Epoch 202/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5433 - acc: 0.5816\n",
      "Epoch 203/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5423 - acc: 0.5818\n",
      "Epoch 204/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5422 - acc: 0.5816\n",
      "Epoch 205/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5419 - acc: 0.5814\n",
      "Epoch 206/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5404 - acc: 0.5816\n",
      "Epoch 207/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5409 - acc: 0.5815\n",
      "Epoch 208/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5397 - acc: 0.5819\n",
      "Epoch 209/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5397 - acc: 0.5818\n",
      "Epoch 210/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5391 - acc: 0.5816\n",
      "Epoch 211/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 1.5400 - acc: 0.5819\n",
      "Epoch 212/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5393 - acc: 0.5817\n",
      "Epoch 213/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5378 - acc: 0.5816\n",
      "Epoch 214/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5392 - acc: 0.5816\n",
      "Epoch 215/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5379 - acc: 0.5817\n",
      "Epoch 216/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5369 - acc: 0.5820\n",
      "Epoch 217/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5372 - acc: 0.5819\n",
      "Epoch 218/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5358 - acc: 0.5818\n",
      "Epoch 219/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5361 - acc: 0.5818\n",
      "Epoch 220/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5369 - acc: 0.5816\n",
      "Epoch 221/500\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 1.5354 - acc: 0.5816\n",
      "Epoch 222/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5358 - acc: 0.5814\n",
      "Epoch 223/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5355 - acc: 0.5816\n",
      "Epoch 224/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5352 - acc: 0.5817\n",
      "Epoch 225/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5342 - acc: 0.5820\n",
      "Epoch 226/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5337 - acc: 0.5823\n",
      "Epoch 227/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5328 - acc: 0.5819\n",
      "Epoch 228/500\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 1.5332 - acc: 0.5822\n",
      "Epoch 229/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5332 - acc: 0.5817\n",
      "Epoch 230/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5329 - acc: 0.5820\n",
      "Epoch 231/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 1.5327 - acc: 0.5821\n",
      "Epoch 232/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5320 - acc: 0.5819\n",
      "Epoch 233/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5321 - acc: 0.5820\n",
      "Epoch 234/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5311 - acc: 0.5820\n",
      "Epoch 235/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5314 - acc: 0.5820\n",
      "Epoch 236/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5307 - acc: 0.5820\n",
      "Epoch 237/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 1.5300 - acc: 0.5820\n",
      "Epoch 238/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5301 - acc: 0.5820\n",
      "Epoch 239/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5286 - acc: 0.5819\n",
      "Epoch 240/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 1.5301 - acc: 0.5819\n",
      "Epoch 241/500\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 1.5297 - acc: 0.5820\n",
      "Epoch 242/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5294 - acc: 0.5820\n",
      "Epoch 243/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 1.5289 - acc: 0.5819\n",
      "Epoch 244/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5280 - acc: 0.5819\n",
      "Epoch 245/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5292 - acc: 0.5818\n",
      "Epoch 246/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5285 - acc: 0.5822\n",
      "Epoch 247/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5284 - acc: 0.5821\n",
      "Epoch 248/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5271 - acc: 0.5819\n",
      "Epoch 249/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5272 - acc: 0.5820\n",
      "Epoch 250/500\n",
      "50/50 [==============================] - 8s 164ms/step - loss: 1.5269 - acc: 0.5819\n",
      "Epoch 251/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5266 - acc: 0.5820\n",
      "Epoch 252/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5264 - acc: 0.5821\n",
      "Epoch 253/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5274 - acc: 0.5820\n",
      "Epoch 254/500\n",
      "50/50 [==============================] - 8s 170ms/step - loss: 1.5260 - acc: 0.5819\n",
      "Epoch 255/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5261 - acc: 0.5823\n",
      "Epoch 256/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5257 - acc: 0.5820\n",
      "Epoch 257/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5244 - acc: 0.5823\n",
      "Epoch 258/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5259 - acc: 0.5820\n",
      "Epoch 259/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5253 - acc: 0.5820\n",
      "Epoch 260/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5242 - acc: 0.5821\n",
      "Epoch 261/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5245 - acc: 0.5823\n",
      "Epoch 262/500\n",
      "50/50 [==============================] - 9s 172ms/step - loss: 1.5246 - acc: 0.5820\n",
      "Epoch 263/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5237 - acc: 0.5822\n",
      "Epoch 264/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5233 - acc: 0.5824\n",
      "Epoch 265/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5243 - acc: 0.5820\n",
      "Epoch 266/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5242 - acc: 0.5822\n",
      "Epoch 267/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5232 - acc: 0.5823\n",
      "Epoch 268/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5232 - acc: 0.5823\n",
      "Epoch 269/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5224 - acc: 0.5822\n",
      "Epoch 270/500\n",
      "50/50 [==============================] - 8s 165ms/step - loss: 1.5217 - acc: 0.5824\n",
      "Epoch 271/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5217 - acc: 0.5824\n",
      "Epoch 272/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5215 - acc: 0.5823\n",
      "Epoch 273/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5215 - acc: 0.5822\n",
      "Epoch 274/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5215 - acc: 0.5823\n",
      "Epoch 275/500\n",
      "50/50 [==============================] - 9s 171ms/step - loss: 1.5220 - acc: 0.5823\n",
      "Epoch 276/500\n",
      "50/50 [==============================] - 8s 168ms/step - loss: 1.5200 - acc: 0.5826\n",
      "Epoch 277/500\n",
      "50/50 [==============================] - 8s 163ms/step - loss: 1.5205 - acc: 0.5824\n",
      "Epoch 278/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5197 - acc: 0.5825\n",
      "Epoch 279/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5184 - acc: 0.5823\n",
      "Epoch 280/500\n",
      "50/50 [==============================] - 8s 166ms/step - loss: 1.5181 - acc: 0.5824\n",
      "Epoch 281/500\n",
      "50/50 [==============================] - 8s 167ms/step - loss: 1.5179 - acc: 0.5827\n",
      "Epoch 282/500\n",
      "50/50 [==============================] - 8s 169ms/step - loss: 1.5155 - acc: 0.5835\n",
      "Epoch 283/500\n",
      "43/50 [========================>.....] - ETA: 1s - loss: 1.5147 - acc: 0.5831"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-34cb44156727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# /media/MyDataStor1/mshaikh2/project_xray/notebooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparallel_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_datagen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# /media/MyDataStor1/mshaikh2/project_xray/notebooks\n",
    "es = EarlyStopping(mode='auto',monitor='loss',patience=50,min_delta=0.0005)\n",
    "hist = parallel_model.fit_generator(t_datagen,epochs=500,verbose=1,steps_per_epoch=50,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hist.history)\n",
    "df.to_csv(\"graphs/sa_gap_8attn_conv_bn_res_add_metrics.csv\")\n",
    "plot1,plot2 = df.plot(subplots=True, figsize=(5,10))\n",
    "fig1 = plot1.get_figure()\n",
    "fig2 = plot2.get_figure()\n",
    "fig1.savefig(\"graphs/sa_gap_8attn_conv_bn_res_add_acc.png\")\n",
    "fig2.savefig(\"graphs/sa_gap_8attn_conv_bn_res_add_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
