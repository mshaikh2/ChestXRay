{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import plot_model,multi_gpu_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input,GlobalAveragePooling2D,Layer,InputSpec\n",
    "from keras.layers.core import Dense,Flatten,Dropout\n",
    "from keras.optimizers import Adam,SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard\n",
    "from keras.utils import np_utils\n",
    "import keras.backend as K\n",
    "import keras.layers as kl\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "import pickle\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# from AttentionModule import SelfAttention, SoftAttention\n",
    "import os\n",
    "visible_gpu_devices = '1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=visible_gpu_devices\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "import AttentionMed as AM\n",
    "from importlib import reload\n",
    "reload(AM)\n",
    "from time import time,localtime,strftime\n",
    "from ipywidgets import IntProgress\n",
    "# from coord import CoordinateChannel2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpu=len(visible_gpu_devices.split(','))\n",
    "n_cpu=1\n",
    "tf_config= tf.ConfigProto(device_count = {'GPU': n_gpu , 'CPU': n_cpu})\n",
    "tf_config.gpu_options.allow_growth=True\n",
    "s=tf.Session(config=tf_config)\n",
    "K.set_session(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 40\n",
    "vocab_size = 100\n",
    "embedding_size = 300\n",
    "elu_alpha = 1.0\n",
    "bottleneck_units = 512\n",
    "batchsize = 256\n",
    "def temp_datagen(batchsize=1):\n",
    "    counter=0\n",
    "    while True:\n",
    "        inp = np.random.randint(1,vocab_size,(batchsize,timesteps,))\n",
    "        out = inp.copy()\n",
    "        out[:,::2]=0\n",
    "        out = np_utils.to_categorical(out,num_classes=101)\n",
    "        counter+=1\n",
    "        if batchsize==counter:\n",
    "            yield inp,out\n",
    "            counter=0\n",
    "t_datagen = temp_datagen(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[61, 82, 46,  6, 69, 34, 30, 46, 18, 95, 48, 54,  1, 11, 41, 73,\n",
       "         61, 74, 68, 29, 51, 61, 15, 73, 66, 56, 68, 68, 79, 65, 74,  3,\n",
       "         40, 40, 91, 26, 92, 22, 62, 53],\n",
       "        [41, 95, 95, 73, 72, 74, 78, 88, 72, 15, 93, 84, 97, 37, 90, 60,\n",
       "         70,  1, 96, 67, 12, 84, 55, 82, 89,  7, 28, 34, 20, 70, 54, 94,\n",
       "         86, 98, 74, 85, 15,  5,  2, 21]]),\n",
       " array([[ 0, 82,  0, ..., 22,  0, 53],\n",
       "        [ 0, 95,  0, ...,  5,  0, 21],\n",
       "        [ 0, 73,  0, ..., 46,  0, 77],\n",
       "        ...,\n",
       "        [ 0,  3,  0, ..., 37,  0, 51],\n",
       "        [ 0,  3,  0, ..., 66,  0, 51],\n",
       "        [ 0,  6,  0, ..., 70,  0, 19]]),\n",
       " (256, 40, 101))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x,t_y=next(t_datagen)\n",
    "t_x[:2],t_y.argmax(-1),t_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "w2v_emb (Embedding)             (None, 40, 300)      30300       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 40, 512)      1665024     w2v_emb[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 40, 512)      2048        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "gap (GlobalAveragePooling1D)    (None, 512)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "a1 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "a2 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "a3 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "a4 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "a5 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "a6 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "a7 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "a8 (Attention)                  [(None, 1, 512), (1, 1050624     gap[0][0]                        \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 1, 512)       2048        a1[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 512)       2048        a2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 512)       2048        a3[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 512)       2048        a4[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1, 512)       2048        a5[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 1, 512)       2048        a6[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 1, 512)       2048        a7[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 1, 512)       2048        a8[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "concat_attn (Concatenate)       (None, 1, 4096)      0           batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "                                                                 batch_normalization_4[0][0]      \n",
      "                                                                 batch_normalization_5[0][0]      \n",
      "                                                                 batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "                                                                 batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "condense_attention1d_1 (Condens (None, 1, 512)       2097664     concat_attn[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "attention_squeeze (Lambda)      (None, 512)          0           condense_attention1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "residual_combine (ResidualCombi [(None, 512), (1,),  2           gap[0][0]                        \n",
      "                                                                 attention_squeeze[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 512)          2048        residual_combine[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 40, 512)      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 40, 512)      2099200     repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 40, 101)      51813       lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 14,369,475\n",
      "Trainable params: 14,359,235\n",
      "Non-trainable params: 10,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "units = 512\n",
    "\n",
    "inp = kl.Input(shape=(timesteps,))\n",
    "emb = kl.Embedding(vocab_size+1, embedding_size, mask_zero=False, name='w2v_emb')(inp)\n",
    "encoder = kl.LSTM(units,activation='relu',return_sequences=True)(emb)\n",
    "encoder = kl.BatchNormalization()(encoder)\n",
    "# encoder,sa_maps = AM.SelfAttention(ch=512)(encoder)\n",
    "\n",
    "# context_vector = kl.LSTM(512,activation='relu',return_sequences=False)(encoder)\n",
    "context_vector = kl.GlobalAveragePooling1D(name='gap')(encoder)\n",
    "\n",
    "\n",
    "acv_1,a_map_1 = AM.Attention(ch=512,timesteps=1,name='a1')([context_vector,encoder])\n",
    "acv_1 = kl.BatchNormalization()(acv_1)\n",
    "\n",
    "acv_2,a_map_2 = AM.Attention(ch=512,timesteps=1,name='a2')([context_vector,encoder])\n",
    "acv_2 = kl.BatchNormalization()(acv_2)\n",
    "\n",
    "acv_3,a_map_3 = AM.Attention(ch=512,timesteps=1,name='a3')([context_vector,encoder])\n",
    "acv_3 = kl.BatchNormalization()(acv_3)\n",
    "\n",
    "acv_4,a_map_4 = AM.Attention(ch=512,timesteps=1,name='a4')([context_vector,encoder])\n",
    "acv_4 = kl.BatchNormalization()(acv_4)\n",
    "\n",
    "acv_5,a_map_5 = AM.Attention(ch=512,timesteps=1,name='a5')([context_vector,encoder])\n",
    "acv_5 = kl.BatchNormalization()(acv_5)\n",
    "\n",
    "acv_6,a_map_6 = AM.Attention(ch=512,timesteps=1,name='a6')([context_vector,encoder])\n",
    "acv_6 = kl.BatchNormalization()(acv_6)\n",
    "\n",
    "acv_7,a_map_7 = AM.Attention(ch=512,timesteps=1,name='a7')([context_vector,encoder])\n",
    "acv_7 = kl.BatchNormalization()(acv_7)\n",
    "\n",
    "acv_8,a_map_8 = AM.Attention(ch=512,timesteps=1,name='a8')([context_vector,encoder])\n",
    "acv_8 = kl.BatchNormalization()(acv_8)\n",
    "\n",
    "concat_attention = kl.Concatenate(name='concat_attn')([acv_1,acv_2,acv_3,acv_4,acv_5,acv_6,acv_7,acv_8])\n",
    "concat_attention = AM.CondenseAttention1D(ch_in=int(concat_attention.shape[-1]),ch_out=512)(concat_attention)\n",
    "concat_attention = kl.Lambda(lambda x:K.squeeze(x,axis=1),name=\"attention_squeeze\")(concat_attention)\n",
    "concat_attention, g1, g2 = AM.ResidualCombine(method='add',name='residual_combine')([context_vector, concat_attention])\n",
    "concat_attention = kl.BatchNormalization()(concat_attention)\n",
    "# acv_combine = kl.Activation('relu')(concat_attention)\n",
    "# acv_combine = kl.Lambda(lambda x:K.expand_dims(x,axis=1),name=\"Expand\")(acv_combine)\n",
    "# acv_combine = kl.Conv1D(filters=bottleneck_units,kernel_size=1,strides=1,padding='same')(acv_combine)\n",
    "# acv_combine = kl.BatchNormalization()(acv_combine)\n",
    "# acv_combine = kl.Lambda(lambda x:K.squeeze(x,axis=1),name=\"Squeeze2\")(acv_combine)\n",
    "\n",
    "# # # # print(attended_context_vector)\n",
    "repeat_context_vector = kl.RepeatVector(n=timesteps)(concat_attention)\n",
    "decoder = kl.LSTM(512,activation='relu',return_sequences=True)(repeat_context_vector)\n",
    "decoder = kl.BatchNormalization()(decoder)\n",
    "# # AM.Attention(ch=1)([])\n",
    "decoder_out = kl.TimeDistributed(Dense(vocab_size+1,activation='softmax'))(decoder)\n",
    "model = Model(inp,decoder_out)\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer=Adam(lr=0.001))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 17s 337ms/step - loss: 3.0897 - acc: 0.4892\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 9s 189ms/step - loss: 2.8164 - acc: 0.5000\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 2.6545 - acc: 0.5024\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 2.0516 - acc: 0.5286\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 10s 198ms/step - loss: 1.9765 - acc: 0.5316\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 10s 200ms/step - loss: 1.9483 - acc: 0.5319\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 10s 197ms/step - loss: 1.9274 - acc: 0.5308\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 10s 200ms/step - loss: 1.9101 - acc: 0.5304\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 10s 194ms/step - loss: 1.8970 - acc: 0.5299\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 10s 197ms/step - loss: 1.8823 - acc: 0.5298\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 10s 204ms/step - loss: 1.8713 - acc: 0.5294\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 10s 199ms/step - loss: 1.8595 - acc: 0.5287\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 10s 191ms/step - loss: 1.8472 - acc: 0.5287\n",
      "Epoch 14/50\n",
      "31/50 [=================>............] - ETA: 3s - loss: 1.8388 - acc: 0.5283"
     ]
    }
   ],
   "source": [
    "# /media/MyDataStor1/mshaikh2/project_xray/notebooks\n",
    "hist = model.fit_generator(t_datagen,epochs=100,verbose=1,steps_per_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(hist.history)\n",
    "df.to_csv(\"graphs/gap_8attn_elu_conv_bn_res_add_metrics.csv\")\n",
    "plot1,plot2 = df.plot(subplots=True, figsize=(5,10))\n",
    "fig1 = plot1.get_figure()\n",
    "fig2 = plot2.get_figure()\n",
    "fig1.savefig(\"graphs/gap_8attn_elu_conv_bn_res_add_acc.png\")\n",
    "fig2.savefig(\"graphs/gap_8attn_elu_conv_bn_res_add_loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
